{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7302e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6206288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.is_available())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cfe79",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b01c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuyển đổi nhãn phù hợp với định dạng yolo\n",
    "def convert_labels_to_yolo_format (boxes, grid_size=7, num_classes=2):\n",
    "    label = torch.zeros((grid_size, grid_size, 5 * 2 + num_classes))\n",
    "    \n",
    "    for box in boxes:\n",
    "        cls_id, x, y, w, h = box\n",
    "        cell_x, cell_y = int(x * grid_size), int(y * grid_size) # ô chịu trách nhiệm có tâm đối tượng\n",
    "        cell_x_frac, cell_y_frac = x * grid_size - cell_x, y * grid_size - cell_y\n",
    "        label[cell_x, cell_y, :5] = torch.tensor([cell_x_frac, cell_y_frac, w, h, 1.0], dtype=torch.float32)\n",
    "        label[cell_x, cell_y, 5:10] = torch.tensor([cell_x_frac, cell_y_frac, w, h, 0.0], dtype=torch.float32)\n",
    "        label[cell_x, cell_y, 10 + int(cls_id)] = 1.0\n",
    "\n",
    "    return label # (7, 7, 5 * 2 + 2)\n",
    "\n",
    "# định nghĩa tập dữ liệu phù hợp với yolo\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__ (self, img_dir, label_dir, resize):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg'))]\n",
    "        self.resize = resize\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "\n",
    "        # ảnh\n",
    "        # bgr -> rgb -> resize -> normalize\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1)) # (h, w, c) -> (c, h, w)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # nhãn\n",
    "        boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls_id, x, y, w, h = map(float, line.strip().split()[:5])\n",
    "                    boxes.append([cls_id, x, y, w, h])\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 5))\n",
    "        yolo_target = convert_labels_to_yolo_format(boxes)\n",
    "\n",
    "        return image, yolo_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = './data/fire/train/images'\n",
    "train_label_dir = './data/fire/train/labels'\n",
    "new_size=448\n",
    "\n",
    "train_dataset = YoloDataset(\n",
    "    img_dir=train_img_dir,\n",
    "    label_dir=train_label_dir,\n",
    "    resize=(new_size, new_size)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e3bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng ảnh: 16\n",
      "Kích thước ảnh: torch.Size([3, 448, 448])\n",
      "Kích thước nhãn: torch.Size([7, 7, 12])\n"
     ]
    }
   ],
   "source": [
    "# kiểm tra Dataloader\n",
    "images, targets = next(iter(train_loader))\n",
    "print(f\"Số lượng ảnh: {len(images)}\")\n",
    "print(f\"Kích thước ảnh: {images[0].shape}\")\n",
    "print(f\"Kích thước nhãn: {targets[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # kiểm tra Dataloader\n",
    "# images, targets = next(iter(train_loader))\n",
    "# images = [img.to(device) for img in images]\n",
    "# targets = [tgt.to(device) for tgt in targets]\n",
    "# print(images[0].device)\n",
    "# print(targets[0].device)\n",
    "\n",
    "# num_imgs = len(images)\n",
    "# cols = 4\n",
    "# rows = (num_imgs + cols - 1) // cols\n",
    "\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(rows * 4, cols * 4))\n",
    "\n",
    "# for i, (image, target) in enumerate(zip(images, targets)):\n",
    "#     image_pil = TF.to_pil_image(image)\n",
    "#     image_cv = np.array(image_pil)[:,:,::-1].copy()\n",
    "#     h, w = image_cv.shape[:2]\n",
    "\n",
    "#     for box in target:\n",
    "#         cls_id, x, y, w_b, h_b = target[..., 0:5]\n",
    "#         xmin = int((x - w_b/2) * w)\n",
    "#         ymin = int((y - h_b/2) * h)\n",
    "#         xmax = int((x + w_b/2) * w)\n",
    "#         ymax = int((y + h_b/2) * h)\n",
    "#         cv2.rectangle(image_cv, (xmin, ymin), (xmax, ymax), color=(0,255,0), thickness=5)\n",
    "\n",
    "#         label = f\"{int(cls_id)}\"\n",
    "#         cv2.putText(image_cv, label, (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "\n",
    "#     r, c = divmod(i, cols)\n",
    "#     ax = axes[r, c] \n",
    "#     ax.imshow(image_cv[:,:,::-1])\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f\"Image {i}\")\n",
    "\n",
    "\n",
    "# for j in range(num_imgs, rows * cols):\n",
    "#     r, c = divmod(j, cols)\n",
    "#     ax = axes[r, c] \n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc6730",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4bd46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\admin\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\admin\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "backbone = models.vgg16(pretrained=True) # input: (Batch, 448, 448, 3)\n",
    "backbone = torch.nn.Sequential(*list(backbone.children())[:-2]) # output: (Batch, 512, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e9f3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(torch.nn.Module):\n",
    "    def __init__ (self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone # sequential\n",
    "\n",
    "        # head\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 1024, kernel_size=3, padding=1), # mở rộng đặc trưng\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Conv2d(1024, 1024, kernel_size=3, padding=1), # đi sâu hơn vào đặc trưng\n",
    "            torch.nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(1024 * 14 * 14, 4096), # kích thước feature map 14*14\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Linear(4096, 7 * 7 * 12) # lưới 7*7*(2 lớp + 5 tham số * 2 boxes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.view(-1, 7, 7, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574604d6",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a29eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    # box1: Tensor shape (N, 4), N là số lượng ô có object\n",
    "    # box format: (xc, yc, w, h)\n",
    "    box1_xy1 = box1[..., :2] - box1[..., 2:] / 2\n",
    "    box1_xy2 = box1[..., :2] + box1[..., 2:] / 2\n",
    "    box2_xy1 = box2[..., :2] - box2[..., 2:] / 2\n",
    "    box2_xy2 = box2[..., :2] + box2[..., 2:] / 2\n",
    "\n",
    "    # Intersection\n",
    "    inter_xy1 = torch.max(box1_xy1, box2_xy1)\n",
    "    inter_xy2 = torch.min(box1_xy2, box2_xy2)\n",
    "    inter_wh = (inter_xy2 - inter_xy1).clamp(min=0)\n",
    "    inter_area = inter_wh[..., 0] * inter_wh[..., 1]\n",
    "\n",
    "    # Area\n",
    "    box1_area = (box1_xy2[..., 0] - box1_xy1[..., 0]) * (box1_xy2[..., 1] - box1_xy1[..., 1])\n",
    "    box2_area = (box2_xy2[..., 0] - box2_xy1[..., 0]) * (box2_xy2[..., 1] - box2_xy1[..., 1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / (union_area + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02169da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(preds, targets, lambda_coord=5, lambda_noobj=0.5):\n",
    "    \"\"\"\n",
    "    Hàm loss cho YOLOv1\n",
    "    Đầu vào:\n",
    "        predictions (Tensor): Shape (batch, 7, 7, 12) - Dự đoán của mô hình\n",
    "        targets (Tensor): Shape (batch, 7, 7, 12) - Ground truth\n",
    "        lambda_coord (float): Trọng số cho localization loss (tọa độ và kích thước có độ quan trọng lớn hơn)\n",
    "        lambda_noobj (float): Trọng số cho confidence loss của box không có vật thể\n",
    "    Đầu ra:\n",
    "        total_loss (Tensor): Tổng loss\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = preds.size[0]\n",
    "    S = 7 # lưới\n",
    "    B = 2 # anchor boxes mỗi cell\n",
    "\n",
    "    pred_boxes = preds[..., :10].view(batch_size, S, S, B, 5) # (x, y, w, h, conf) * 2\n",
    "    target_boxes = targets[..., :10].view(batch_size, S, S, B, 5)\n",
    "    pred_classes = preds[..., 10:]\n",
    "    target_classes = targets[..., 10:]\n",
    "\n",
    "    # đánh dấu ô lưới chứa đối tượng \n",
    "    obj_mask = target_boxes[..., 0, 4] > 0\n",
    "    noobj_mask = ~obj_mask\n",
    "\n",
    "    # gán trách nhiệm box cho ô chứa đối tượng dự đoán box nào có giá trị iou tốt nhất\n",
    "    pred_box0 = pred_boxes[..., 0, :4][obj_mask]\n",
    "    pred_box1 = pred_boxes[..., 1, :4][obj_mask]\n",
    "    target_box = target_boxes[..., 0, :4][obj_mask]\n",
    "    iou0 = compute_iou(pred_box0, target_box)\n",
    "    iou1 = compute_iou(pred_box1, target_box)\n",
    "\n",
    "    responsible_mask = iou0 > iou1\n",
    "\n",
    "    pred_box = torch.where(\n",
    "            responsible_mask.unsqueeze(-1),\n",
    "            pred_box0,\n",
    "            pred_box1\n",
    "        )\n",
    "    \n",
    "    # 1. localization loss\n",
    "    coord_loss = 0\n",
    "    if obj_mask.sum() > 0:\n",
    "        # sai số tọa độ\n",
    "        xy_loss = F.mse_loss(pred_box[..., 0:2], target_box[..., 0:2], reduction='sum')\n",
    "\n",
    "        # sai số kích thước\n",
    "        pred_wh = torch.sqrt(pred_box[..., 2:4].clamp(min=0))\n",
    "        target_wh = torch.sqrt(target_box[..., 2:4])\n",
    "        wh_loss = F.mse_loss(pred_wh, target_wh, reduction='sum')\n",
    "\n",
    "        coord_loss = lambda_coord * (xy_loss + wh_loss)\n",
    "\n",
    "    # 2. confidence loss\n",
    "    # box chứa vật thể\n",
    "    conf_obj_loss = 0\n",
    "    if obj_mask.sum() > 0:\n",
    "        pred_conf_obj = pred_box[..., 4]\n",
    "        target_conf_obj = target_box[..., 4][obj_mask]\n",
    "        conf_obj_loss = F.mse_loss(pred_conf_obj, target_conf_obj, reduction='sum')\n",
    "\n",
    "    # box không chứa vật thể (tất cả box) \n",
    "    pred_conf_noobj = pred_boxes[..., :, 4][noobj_mask.unsqueeze(-1).expand(-1, -1, -1, B)].view(-1)\n",
    "    target_conf_noobj = target_boxes[..., :, 4][noobj_mask.unsqueeze(-1).expand(-1, -1, -1, B)].view(-1)\n",
    "    conf_noobj_loss = lambda_noobj * F.mse_loss(pred_conf_noobj, target_conf_noobj, reduction='sum')\n",
    "\n",
    "    confidence_loss = conf_obj_loss + conf_noobj_loss\n",
    "\n",
    "    # 3. class loss\n",
    "    class_loss = 0\n",
    "    if obj_mask.sum() > 0:\n",
    "        pred_class = pred_classes[obj_mask]\n",
    "        target_class = target_classes[obj_mask]\n",
    "        class_loss = F.mse_loss(pred_class, target_class, reduction='sum')\n",
    "\n",
    "    # tổng loss\n",
    "    total_loss = coord_loss + confidence_loss + class_loss\n",
    "    total_loss /= batch_size  # Chuẩn hóa theo batch size\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa067ac8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b287cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv1(backbone).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68bd568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 7, 7, 12])\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "input = torch.randn(1,3,448,448).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffa0c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor '__init__' of 'super' object needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLOv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m      4\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mYOLOv1.__init__\u001b[1;34m(self, backbone)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m (\u001b[38;5;28mself\u001b[39m, backbone):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m backbone \u001b[38;5;66;03m# sequential\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# head\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: descriptor '__init__' of 'super' object needs an argument"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [tgt.to(device) for tgt in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = yolo_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    epoch_losses.append(average_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d05e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'yolov1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07743dad",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
