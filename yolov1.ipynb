{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302e5e9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6206288",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cfe79",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b01c37",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# chuyển đổi nhãn phù hợp với định dạng yolo\n",
    "def convert_labels_to_yolo_format (boxes, grid_size=7, num_classes=2):\n",
    "    label = torch.zeros((grid_size, grid_size, 5 * 2 + num_classes))\n",
    "    \n",
    "    for box in boxes:\n",
    "        cls_id, x, y, w, h = box\n",
    "        cell_x, cell_y = int(x * grid_size), int(y * grid_size) # ô chịu trách nhiệm có tâm đối tượng\n",
    "        cell_x_frac, cell_y_frac = x * grid_size - cell_x, y * grid_size - cell_y\n",
    "        label[cell_x, cell_y, :5] = torch.tensor([cell_x_frac, cell_y_frac, w, h, 1.0], dtype=torch.float32)\n",
    "        label[cell_x, cell_y, 5:10] = torch.tensor([cell_x_frac, cell_y_frac, w, h, 0.0], dtype=torch.float32)\n",
    "        label[cell_x, cell_y, 10 + int(cls_id)] = 1.0\n",
    "\n",
    "    return label # (7, 7, 5 * 2 + 2)\n",
    "\n",
    "# định nghĩa tập dữ liệu phù hợp với yolo\n",
    "class YoloDataset(Dataset):\n",
    "    def __init__ (self, img_dir, label_dir, resize):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg'))]\n",
    "        self.resize = resize\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "\n",
    "        # ảnh\n",
    "        # bgr -> rgb -> resize -> normalize\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1)) # (h, w, c) -> (c, h, w)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # nhãn\n",
    "        boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls_id, x, y, w, h = map(float, line.strip().split()[:5])\n",
    "                    boxes.append([cls_id, x, y, w, h])\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 5))\n",
    "        yolo_target = convert_labels_to_yolo_format(boxes)\n",
    "\n",
    "        return image, yolo_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686a3e1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_img_dir = '/kaggle/input/yolofire/train/images'\n",
    "train_label_dir = '/kaggle/input/yolofire/train/labels'\n",
    "val_img_dir = '/kaggle/input/yolofire/valid/images'\n",
    "val_label_dir = '/kaggle/input/yolofire/valid/labels'\n",
    "new_size=224\n",
    "\n",
    "train_dataset = YoloDataset(\n",
    "    img_dir=train_img_dir,\n",
    "    label_dir=train_label_dir,\n",
    "    resize=(new_size, new_size)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n",
    "\n",
    "val_dataset = YoloDataset(\n",
    "    img_dir=val_img_dir,\n",
    "    label_dir=val_label_dir,\n",
    "    resize=(new_size, new_size)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3bc18",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# kiểm tra Dataloader\n",
    "images, targets = next(iter(train_loader))\n",
    "print(f\"Số lượng ảnh: {len(images)}\")\n",
    "print(f\"Kích thước ảnh: {images[0].shape}\")\n",
    "print(f\"Kích thước nhãn: {targets[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477e2b1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # in ảnh từ Dataloader\n",
    "# images, targets = next(iter(train_loader))\n",
    "# images = [img.to(device) for img in images]\n",
    "# targets = [tgt.to(device) for tgt in targets]\n",
    "# print(images[0].device)\n",
    "# print(targets[0].device)\n",
    "\n",
    "# num_imgs = len(images)\n",
    "# cols = 4\n",
    "# rows = (num_imgs + cols - 1) // cols\n",
    "\n",
    "# fig, axes = plt.subplots(rows, cols, figsize=(rows * 4, cols * 4))\n",
    "\n",
    "# for i, (image, target) in enumerate(zip(images, targets)):\n",
    "#     image_pil = TF.to_pil_image(image) \n",
    "#     image_cv = np.array(image_pil)[:,:,::-1].copy()\n",
    "#     H, W = image_cv.shape[:2]\n",
    "#     S = target.shape[0]\n",
    "\n",
    "#     for ix in range(S):\n",
    "#         for iy in range(S):\n",
    "#             cell = target[ix, iy]\n",
    "\n",
    "#             conf = cell[4].item()\n",
    "\n",
    "#             if conf == 0: continue\n",
    "\n",
    "#             x_frac, y_frac, w_frac, h_frac = cell[0:4].tolist()\n",
    "#             cls_id = int(torch.argmax(cell[10:]))\n",
    "\n",
    "#             abs_x = (ix + x_frac) / S\n",
    "#             abs_y = (iy + y_frac) / S\n",
    "\n",
    "#             xmin = int((abs_x - w_frac/2) * W)\n",
    "#             ymin = int((abs_y - h_frac/2) * H)\n",
    "#             xmax = int((abs_x + w_frac/2) * W)\n",
    "#             ymax = int((abs_y + h_frac/2) * H)\n",
    "\n",
    "#             cv2.rectangle(image_cv, (xmin, ymin), (xmax, ymax), (0,255,0), 2)\n",
    "\n",
    "#             label = f\"{int(cls_id)}\"\n",
    "#             cv2.putText(image_cv, label, (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "\n",
    "#     r, c = divmod(i, cols)\n",
    "#     ax = axes[r, c]\n",
    "#     ax.imshow(image_cv[:, :, ::-1])\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f\"Image {i}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc6730",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4bd46d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "backbone = models.mobilenet_v2(pretrained=True).features # input: (Batch, 224, 224, 3), output: (Batch, 1280, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f3c90",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class YOLOv1(torch.nn.Module):\n",
    "    def __init__ (self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone # sequential\n",
    "\n",
    "        # head\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1280, 2048, kernel_size=3, padding=1), # mở rộng đặc trưng\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Conv2d(2048, 1024, kernel_size=3, padding=1),\n",
    "            torch.nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(1024 * 7 * 7, 2048), # kích thước feature map 7*7\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.LeakyReLU(0.1),\n",
    "            torch.nn.Linear(2048, 7 * 7 * 12) # lưới 7*7*(2 lớp + 5 tham số * 2 boxes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.view(-1, 7, 7, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574604d6",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29eb59",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    # box1: Tensor shape (N, 4), N là số lượng ô có object\n",
    "    # box format: (xc, yc, w, h)\n",
    "    box1_xy1 = box1[..., :2] - box1[..., 2:] / 2\n",
    "    box1_xy2 = box1[..., :2] + box1[..., 2:] / 2\n",
    "    box2_xy1 = box2[..., :2] - box2[..., 2:] / 2\n",
    "    box2_xy2 = box2[..., :2] + box2[..., 2:] / 2\n",
    "\n",
    "    # Intersection\n",
    "    inter_xy1 = torch.max(box1_xy1, box2_xy1)\n",
    "    inter_xy2 = torch.min(box1_xy2, box2_xy2)\n",
    "    inter_wh = (inter_xy2 - inter_xy1).clamp(min=0)\n",
    "    inter_area = inter_wh[..., 0] * inter_wh[..., 1]\n",
    "\n",
    "    # Area\n",
    "    box1_area = (box1_xy2[..., 0] - box1_xy1[..., 0]) * (box1_xy2[..., 1] - box1_xy1[..., 1])\n",
    "    box2_area = (box2_xy2[..., 0] - box2_xy1[..., 0]) * (box2_xy2[..., 1] - box2_xy1[..., 1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / (union_area + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02169da6",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def yolo_loss(preds, targets, lambda_coord=5, lambda_noobj=0.5):\n",
    "    batch_size = preds.size(0)\n",
    "    S = 7 # lưới\n",
    "    B = 2 # anchor boxes mỗi cell\n",
    "\n",
    "    pred_boxes = preds[..., :10].view(batch_size, S, S, B, 5) # (x, y, w, h, conf) * 2\n",
    "    target_boxes = targets[..., :10].view(batch_size, S, S, B, 5)\n",
    "    pred_classes = preds[..., 10:]\n",
    "    target_classes = targets[..., 10:]\n",
    "\n",
    "    # đánh dấu ô lưới chứa đối tượng \n",
    "    obj_mask = target_boxes[..., 0, 4] > 0\n",
    "    noobj_mask = ~obj_mask\n",
    "\n",
    "    # gán trách nhiệm box cho ô chứa đối tượng dự đoán box nào có giá trị iou tốt nhất\n",
    "    pred_box0 = pred_boxes[..., 0, :5][obj_mask]\n",
    "    pred_box1 = pred_boxes[..., 1, :5][obj_mask]\n",
    "    target_box = target_boxes[..., 0, :5][obj_mask]\n",
    "    iou0 = compute_iou(pred_box0[..., :4], target_box[..., :4])\n",
    "    iou1 = compute_iou(pred_box1[..., :4], target_box[..., :4])\n",
    "\n",
    "\n",
    "    responsible_mask = iou0 > iou1\n",
    "\n",
    "    pred_box = torch.where(\n",
    "            responsible_mask.unsqueeze(-1),\n",
    "            pred_box0,\n",
    "            pred_box1\n",
    "        )\n",
    "    \n",
    "    # 1. localization loss\n",
    "    coord_loss = 0\n",
    "    if obj_mask.sum() > 0:\n",
    "        # sai số tọa độ\n",
    "        xy_loss = F.mse_loss(pred_box[..., 0:2], target_box[..., 0:2], reduction='sum')\n",
    "\n",
    "        # sai số kích thước\n",
    "        pred_wh = torch.sqrt(pred_box[..., 2:4].clamp(min=0))\n",
    "        target_wh = torch.sqrt(target_box[..., 2:4])\n",
    "        wh_loss = F.mse_loss(pred_wh, target_wh, reduction='sum').clamp(min=0)\n",
    "\n",
    "        coord_loss = lambda_coord * (xy_loss + wh_loss)\n",
    "\n",
    "    # 2. confidence loss\n",
    "    # box chứa vật thể\n",
    "    conf_obj_loss = 0\n",
    "    if obj_mask.sum() > 0:\n",
    "        pred_conf_obj = pred_box[..., 4]\n",
    "        target_conf_obj = target_box[..., 4]\n",
    "        conf_obj_loss = F.mse_loss(pred_conf_obj, target_conf_obj, reduction='sum')\n",
    "\n",
    "    # box không chứa vật thể (tất cả box) \n",
    "    pred_conf_noobj = pred_boxes[..., :, 4][noobj_mask.unsqueeze(-1).expand(-1, -1, -1, B)].view(-1)\n",
    "    target_conf_noobj = target_boxes[..., :, 4][noobj_mask.unsqueeze(-1).expand(-1, -1, -1, B)].view(-1)\n",
    "    conf_noobj_loss = lambda_noobj * F.mse_loss(pred_conf_noobj, target_conf_noobj, reduction='sum')\n",
    "\n",
    "    confidence_loss = conf_obj_loss + conf_noobj_loss\n",
    "\n",
    "    # 3. class loss\n",
    "    class_loss = 0\n",
    "    if obj_mask.sum() > 0:\n",
    "        pred_class = pred_classes[obj_mask]\n",
    "        target_class = target_classes[obj_mask]\n",
    "        class_loss = F.cross_entropy(pred_class, target_class, reduction='sum')\n",
    "\n",
    "    # tổng loss\n",
    "    total_loss = coord_loss + confidence_loss + class_loss\n",
    "    total_loss /= batch_size  # chuẩn hóa theo batch size\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa067ac8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b287cdb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = YOLOv1(backbone).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd568d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check model\n",
    "input = torch.randn(1,3,224,224).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffa0c7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    for images, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} Training', leave=False):\n",
    "        images  = torch.stack([img.to(device) for img in images])\n",
    "        targets = torch.stack([tgt.to(device) for tgt in targets])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = yolo_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    epoch_train_losses.append(avg_train_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} — Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # val \n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} Validation', leave=False):\n",
    "            images  = torch.stack([img.to(device) for img in images])\n",
    "            targets = torch.stack([tgt.to(device) for tgt in targets])\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = yolo_loss(outputs, targets)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    epoch_val_losses.append(avg_val_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} — Val Loss:   {avg_val_loss:.4f}')\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_yolo_fire_smoke.pth')\n",
    "        print(f'  → New best model (Val Loss: {best_val_loss:.4f}), saving checkpoint.')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f'  → No improvement for {epochs_no_improve}/{patience} epochs.')\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs.')\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "if early_stop:\n",
    "    print(f'Training stopped early. Best Val Loss: {best_val_loss:.4f}')\n",
    "else:\n",
    "    print(f'Training completed. Best Val Loss: {best_val_loss:.4f}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_train_losses, label='Train Loss')\n",
    "plt.plot(epoch_val_losses,   label='Val Loss')\n",
    "plt.legend(); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d05e84",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = torch.load('best_yolo_fire_smoke.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c32a8-b448-4734-a47b-8b039adfc419",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9e77f-7eab-4a74-9cc0-179b3151132b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07743dad",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878b139-d773-49ee-bf89-0e92869613ab",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7487369,
     "sourceId": 11909969,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31042,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "yoloenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
